---
copyright: Copyright (c) Runtime Verification, Inc. All Rights Reserved.
---

# Design
The overall design of the new sort inference algorithm (`SortInferencer.java`) is based on the paper [The Simple Essence of Algebraic Subtyping: Principal Type Inference with Subtyping Made Easy](https://infoscience.epfl.ch/record/278576) by Lionel Parreaux. We summarize the relevant parts below, but it's a short and enlightening paper that's worth reading if you want a deeper understanding.

## SimpleSub Summary
The SimpleSub paper describes a type inference algorithm for a lambda calculus with sub-typing. The type system is akin in expressiveness to something like Java-generics, i.e., permitting type variables `ùõº` with bounds `L <: ùõº <: U` (`super` and `extends` in Java).

Notably though, it captures this expressiveness with a very simple type semantics, enabling inferred subtyping constraints to be efficiently reduced to constraints on type variables (e.g. by mutating a type variable's sub-/super-type bounds throughout the inference process). As well, the results are expressed using "set-theoretic" types, which allow the type constraints to be recorded in an indirect, compact form while also making certain type-simplifications more obvious.

The inferred types have the following syntax
```
ùúè ::= primitive                  // built-ins
    | ùúè ‚Üí ùúè                      // functions
    | { ùëô0 : ùúè ; ...; ùëôùëõ : ùúè }    // structurally-typed records (won't be relevant for us)
    | ùõº                         // type variables
    | ‚ä§                         // top type which is a supertype of all others
    | ‚ä•                         // bottom type which is a subtype of all others
    | ùúè ‚äî ùúè                     // type unions / joins
    | ùúè ‚äì ùúè                     // type intersections / meets
    | ùúáùõº.ùúè                      // recursive types (won't be relevant for us)
```
which is additionally subject to a *polarity* restriction. Informally, for a type `ùúè` which is a syntactic subcomponent of some other type `T`, the polarity of `ùúè` is
- *negative*, if `ùúè` describes a value given as an input to a term with type `T`
- *positive*, if `ùúè` describes a value produced by a term with type `T`

As a concrete example, in a type like `(ùúè0 ‚Üí ùúè1) ‚Üí ùúè2`,
- `ùúè2` is positive, as the term produces the value of type `ùúè2` as an output 
- `ùúè0 ‚Üí ùúè1` is negative, as the value of this type is given as an input to the term
- `ùúè1` is negative, as the value of this type is also given as an input to the term (indirectly via the input function `ùúè0 ‚Üí ùúè1`)
- `ùúè0` is positive, as the term itself must produce this value in order to call the input function `ùúè0 ‚Üí ùúè1`

More formally, we define the type as a whole to be positive, and say
- `ùúè` is negative if either
    - it occurs as the left part of an arrow `ùúè ‚Üí ùúè'` where `ùúè ‚Üí ùúè'` is itself positive, or
    - it occurs as the right part of an arrow `ùúè' ‚Üí ùúè` where `ùúè' ‚Üí ùúè` is itself negative
- `ùúè` is positive otherwise

The polarity restriction on our type syntax then requires that
- type intersections `ùúè ‚äì ùúè` may only occur in negative position
- type unions `ùúè ‚äî ùúè` may only occur in positive position

To understand the motivation for this restriction, consider the subtyping constraints in a program and observe that
- if a type `ùúè` is negative, then it corresponds to an upper bound - the term requires a `ùúè` as input, and therefore can accept any sub-type of `ùúè`
- if a type `ùúè` is positive, then it corresponds to a lower bound - the term produces a `ùúè` as output, which can then be used at any place where some supertype of `ùúè` is expected

Informally then, the polarity restriction enforces that type intersections can only be used for upper bounds and type unions can only be used for lower bounds. In fact, there is an exact correspondence, as conversely any upper/lower bounds can always be encoded by a type intersection/union:
- `ùúè <: ùúè1 and ùúè <: ùúè2 iff ùúè <: ùúè1 ‚äì ùúè2`
- `ùúè1 <: ùúè and ùúè2 <: ùúè iff ùúè1 ‚äî ùúè2 <: ùúè`

In total then, any type variable with bounds `L <: ùõº <: U` can be encoded as a set-theoretic type by 
- replacing every negative instance of `ùõº` with `ùõº ‚äì U` 
- replacing every positive instance of `ùõº` with `ùõº ‚äî L`

Conversely, any set-theoretic type subject to the polarity restriction can be converted back to type variables with bounds by iteratively applying this process in reverse, i.e.,
- replacing every intersection involving a type variable `ùõº ‚äì U` with `ùõº` and recording the bound `ùõº <: U` (introducing a fresh type variable for intersections involving only concrete types)
- replacing every union involving a type variable `ùõº ‚äî L` with `ùõº` and recording the bound `L <: ùõº` (introducing a fresh type variable for unions involving only concrete types)

For example, consider a term like
```
ùúÜùë• . { L = ùë• ‚àí 1 ; R = if ùë• < 0 then 0 else ùë• }
 ```
where `nat <: int`, `- : int ‚Üí int ‚Üí int`, and `0 : nat`. 

Prior to some simplification passes, SimpleSub will infer the type
```
ùõº ‚äì int ‚Üí { L : int ; R : ùõΩ ‚äî nat ‚äî ùõº }
```
which corresponds to the Java-esque type
```
‚ü®ùõº extends int, ùõΩ super nat | ùõº‚ü©(ùõº) ‚Üí { L : int ; R : ùõΩ }
```
After simplification, SimpleSub will produce the final type
```
ùõº ‚äì int ‚Üí { L : int ; R : nat ‚äî ùõº }
```
which corresponds to the Java-esque type
```
‚ü®ùõº super nat extends int‚ü©(ùõº) ‚Üí { L : int ; R : ùõº }
```

### Inference Algorithm
With this background understood, the actual algorithm is quite simple. Below, I provide the algorithms from the paper in Scala with all the parts that are irrelevant to us removed (namely records, let-bindings, and recursive types).

Take our AST as follows:
```
enum Term {
case Lit (value: Int)
case Var (name: String)
case Lam (name: String, rhs: Term)
case App (lhs: Term, rhs: Term)
}
```
The first step is to produce a `SimpleType` which directly records bounds for each variable:
```
enum SimpleType {
case Variable (st: VariableState)
case Primitive (name: String)
case Function (lhs: SimpleType, rhs: SimpleType)
}

class VariableState(var lowerBounds: List[SimpleType],
                    var upperBounds: List[SimpleType])
```
The algorithm proceeds straightforwardly, noting type constraints at each function application:
```
def typeTerm(term: Term)(implicit ctx: Map[String, SimpleType]): SimpleType = term match {
case Lit(n) => Primitive("int")
case Var(name) => ctx.getOrElse(name, err("not found: " + name))
case Lam(name, body) =>
  val param = freshVar
  Function(param, typeTerm(body)(ctx + (name -> param)))
case App(f, a) =>
  val res = freshVar
  constrain(typeTerm(f), Function(typeTerm(a), res))
  res
}
```
The constrain function propagates the newly found sub-typing constraint in a manner that ensures coherence with all previously recorded constraints:
```
def constrain(lhs: SimpleType, rhs: SimpleType)
     (implicit cache: MutSet[(SimpleType, SimpleType)]): Unit = {
if (cache.contains(lhs -> rhs)) return () else cache += lhs -> rhs
(lhs, rhs) match {
  case (Primitive(n0), Primitive(n1)) if subtype(n0, n1) => () // nothing to do
  case (Function(l0, r0), Function(l1, r1)) =>
    constrain(l1, l0); constrain(r0, r1)
  case (Variable(lhs), rhs) =>
    lhs.upperBounds = rhs :: lhs.upperBounds
    lhs.lowerBounds.foreach(constrain(_, rhs))
  case (lhs, Variable(rhs)) =>
    rhs.lowerBounds = lhs :: rhs.lowerBounds
    rhs.upperBounds.foreach(constrain(lhs, _))
  case _ => err("cannot constrain " + lhs + " <: " + rhs)
}}
```
Once the `SimpleType` is inferred, we then go through the process described before of encoding the typing constraints as unions or intersections of the bounds. The end result is a `CompactType`, which denotes either a union or intersection of its contained types depending on the polarity where it occurs:

```
case class CompactType(vars: Set[TypeVariable],
                       prims: Set[PrimType],
                       fun: Option[(CompactType, CompactType)])
```
The implementation of the compact function is straightforward and omitted here.

We then perform two simplifications on these `CompactTypes` to remove unnecessary parametricity. We say that two type variables *co-occur* if they appear in the same union or intersection. The two simplifications are then as follows: 

- If a type variable `ùõº` always co-occurs positively with some other type variable `Œ≤` and vice-versa, then  `ùõº` and `Œ≤` can be unified. The same applies for negative occurrences. For example, the obvious type for an `if-then-else` function `bool ‚Üí ùõº ‚Üí Œ≤ ‚Üí ùõº ‚äî Œ≤` is in fact equivalent to `bool ‚Üí ùõº ‚Üí ùõº ‚Üí ùõº`.

- If a type variable `ùõº` always co-occurs both positively and negatively with some other type `T`, then `ùõº` can be removed. For example, `ùõº ‚äì Int ‚Üí ùõº ‚äî Int` is the same as `Int ‚Üí Int`

See the paper for an explanation of why these simplifications are justified.

The final step of the algorithm is to coalesce each `CompactType` into the final set-theoretic type syntax described initially. Ignoring recursive types, this is just an easy syntactic transformation, so we omit the implementation here (it's also unneeded for our use cases).

## From SimpleSub To Sort Inference

To begin, consider the simplified setting of terms without ambiguities or parametric sorts.

The conceptual translation from K `Term`s to the SimpleSub language is straightforward.

First, given `pr` a `ProductionReference`  whose id is `prId`, sort is `S`, non-terminal sorts are `S1, ..., SK`, and items are `t1, ..., tK`, we note that the typing constraints it induces are the same as any function symbol, so we let
```
translateBody(pr) = prId translateBody(t1) ... translateBody(tK)
```
where
```
prId : S1 ‚Üí ... ‚Üí SK ‚Üí S
```
is taken as a built-in in the SimpleSub language. 

Then, given `t` any `Term` containing variables `x1, ..., xN`, define the translation
```
translate(t) = ùúÜx1. ... ùúÜxN. translateBody(t)
```

When we perform type inference on `translate(t)`, we will obtain a function type of the form `T1 ‚Üí ... ‚Üí TN ‚Üí T`. This tells us that `x1 : T1`, ..., `xN : TN`, and, `term : T`, exactly as needed for sort inference! 

There are a few final caveats, which can be easily addressed:
- In K, we do not actually have a complete type lattice, and the intersections and unions of sorts may not actually exist. If we produce a type with such non-existent sorts, it is simply a type error.
- We do not yet support parametric rules throughout K, and may not ever support bounds on parametric sort variables (although this is something to consider!). We can address this by just monomorphizing with every sort in the poset that satisfies the appropriate bounds for each variable. These different monomorphizations should correspond one-to-one with the different maximal models found in the Z3-based sort inference engine.
- Function, macro, and anywhere rules must be special-case. Conceptually, we can just think of these as a `ProductionReference` to some specialized `KRewrite` whose return sort is exactly the declared sort of the LHS.
- Strict casts require us to unify rather than introduce subtyping constraints.

### Ambiguities
To extend the idea above to handle ambiguities, we note that any parse tree can be divided into unambiguous sub-trees (which we call *slices* in the implementation). The high-level idea is to infer a type for each slice parametric over its contained ambiguities.

Explicitly, given `t` a `Term` thought of as a tree/AST, cut `t` at each ambiguity node. This will produce a number of sub-trees trees, each of whose root node is either the root of `t` or the direct child of some `Ambiguity` occurring in `t`, and each of whose leaves are either a `Constant` or `Ambiguity`.

Given such a slice `t` containing `Ambiguity` nodes with which we associate identifiers `amb1, ..., ambK`, and presuming our overall term contains the variables `x1, ..., xN`, let
```
translateSlice(t) =  ùúÜamb1. ... ùúÜambK. ùúÜx1. ... ùúÜxN. translateBody(t)
```
and extend `translateBody` as above with the rule
```
translateBody(ambi) = ambi x1 ... xN
```
The intuition here is that our translation shows that any `Term` corresponds to a SimpleSub function abstracted over all the contrained variables, so an ambiguity can just be represented as some function variable (whose concrete types are not yet known) which is applied to all these variables.

When we perform inference on `translateSlice(t)`, the resulting type will have the form
```
(A11 ‚Üí ... ‚Üí A1N ‚Üí A1)  ‚Üí ... ‚Üí (AK1 ‚Üí ... ‚Üí AKN ‚Üí AK) ‚Üí T1 ‚Üí ... ‚Üí TN ‚Üí T`.
```
Here, each `Ai` indicates the expected type of the `Ambiguity` associated to the identifier `ambi`.

If an ambiguity's child does not itself contain ambiguities (as will always be true far enough down in the parse tree), we can "fold" that particular child into the type of the parent slice by simple function application, corresponding to choosing that child of the ambiguity for our final parse. 

Specifically, let `t` be the term as above and `c` its child slice. We can infer the type of the function application `translateSlice(t) translateSlice(c)` and will be left with either a type error or a type of the form
```
(A21 ‚Üí ... ‚Üí A2N ‚Üí A1)  ‚Üí ... ‚Üí (AK1 ‚Üí ... ‚Üí AKN ‚Üí AK) ‚Üí T1' ‚Üí ... ‚Üí TN' ‚Üí T'`.
```
which is the specialization of `translateSlice(t)`'s type when picking `c` as the value of the ambiguity. 

We can use this process to then iteratively collapse the tree of slices along all possible paths, starting from the leaves upward. Those paths that encounter type errors during substitution are pruned, while the others are collected into a set of all possible parses along with their inferred types. Unfortunately, this is `O(2^N)` in the worst case, but the hope is that the actual factors involved will be small.
